---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.4.1
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

## Word Counter

<!-- #region -->
First of all, you have to implement the following distributred algorithm to *count* the occurrences of all the words inside a list of documents. In NLP (Natural Language Processing) a document is a text; in this case, each paper is a document.

The algorithm is defined as follows:

- **Map phase**: For each document $D_i$, produce the set of intermediate pairs $(w, \mathrm{cp}_i(w))$, one for each word $w \in D_i$, where $\mathrm{cp}_i(w)$ is the number of occurrences of $w$ in $D_i$. E.g. $('\mathrm{hello}', 3)$.
- **Reduce phase**: For each word $w$, gather all the previous pairs $(w, \mathrm{cp}_i(w))$ and return the final pair $(w, \mathrm{c}(w))$ where $\mathrm{c}(w)$ is the number of occurrences of $w$ for all the documents. In other words:
$$ \mathrm{c}(w) = \sum_{k=1}^n \mathrm{cp}_k(w)$$


1. The algorithm has to be run on the **full-text** of the papers. To get the full text of a paper you have to transform the input data by concatenating the strings contained in the *body-text* fields of the JSON. To perform this transformation I strongly suggest you use the Bag data-structure of DASK. Anyway, if you prefer to implement the algorithm by using the DataFrame structure feel free to do it.

2. The algorithm has to be run several times by changing the *number of workers* and the *number of partitions*. For each run the **execution time** must be registered. Provide a comment on how the computation time over the cluster varies by changing the number of partitions/workers. You have to try with at least $6$ different partition numbers.

3. At the end of the algorithm, analyze the **top words** and see how they are related to viruses and research (for example create a barplot of the top words).

<!-- #endregion -->

```{python}
from dask.distributed import Client

client = Client()
```

```{python}
import dask.bag as db
import os
import json

PATH = os.path.join('data', 'papers_in_json', '*.json')

js = db.read_text(PATH).map(json.loads)
```

```{python}
js = js.repartition(100)
```

```{python}
#string = ''.join([a['text'] for a in data['body_text']])

def joiner(paper):
    return ''.join([paragraph['text'] for paragraph in paper['body_text']])
```

```{python}
# Unire text in body-text
# Rimuovere punteggiatura etc.
# Contare parole (parola, conto)
import glob 

filename = glob.glob(PATH)[0]

with open(filename) as f:
    data = json.loads(f.read())

string = ' '.join([a['text'] for a in data['body_text']]) 
```

```{python}
import re

citations = re.compile('\[[\d]+\]')
numbers = re.compile('((^|\s)[\d]+)')
fixpoints = re.compile('([\.,:?;!](?=\w))')
urls = re.compile('(http(s)?:\/\/.)?(www\.)?[-a-zA-Z0-9@:%._\+~#=]{2,256}\.[a-z]{2,6}\b([-a-zA-Z0-9@:%_\+.~#?&//=]*)')
pattern = re.compile('[^a-zA-Z\d\s]')
spaces = re.compile('\s([\s]+)')
#Remove url

def sanitize_string(s):
    sanitized_string = s.lower()
    sanitized_string = urls.sub('', sanitized_string)
    sanitized_string = citations.sub('', sanitized_string)
    sanitized_string = fixpoints.sub(' ', sanitized_string)
    sanitized_string = pattern.sub('', sanitized_string)
    sanitized_string = numbers.sub('', sanitized_string)
    sanitized_string = spaces.sub('', sanitized_string)
    
    return sanitized_string

s = sanitize_string(string)
```

```{python}
def count_words(s):
    wordlist = {}
    
    for word in s.split():
        if word in wordlist:
            wordlist[word] += 1
        else:
            wordlist[word] = 1
        
    return wordlist
```

```{python}
js = js.map(joiner)
js = js.map(sanitize_string).map(count_words)
```

```{python}
def merge_dictionaries(x, y):
    return {k: x.get(k, 0) + y.get(k, 0) for k in set(x) | set(y)}
    
reduction = js.fold(binop=merge_dictionaries, combine=merge_dictionaries)
```

```{python}
js
```

```{python}
a = reduction.compute() #Fix error of plasmodium, should be 53 or 52
```

```{python}
a['plasmodium']
```

## Worst and Best represented Countries
1. In this part you have to take the documents and convert them in a usable *DataFrame* data structure in order to figure out the countries that are most and less active in the research. To do this you can use the country of the authors. Do the same for the universities (affiliations).

2. Even in this case, do multiple runs by changing the *number of partitions* and *workers* and then describe the behaviour of the timings.

```{python}
PATH = os.path.join('data', 'papers_in_json', '*.json')

js = db.read_text(PATH).map(json.loads)

#metadata => authors => affiliation => location => country

def reformat(paper):
    metadata = paper['metadata'] #dictionary
    
    dataframe = []
    
    for author in metadata['authors']:
#         affiliations = author['affiliation']
        
#         if type(affiliations) is not list:
#             affiliations = list(affiliations)
        
        #for a in affiliations:
        a = author['affiliation']
        
        try:
            country = a['location']['country']
        except KeyError:
            country = 'Missing'
        
        try:
            institution = a['institution']
        except KeyError:
            institution = 'Missing'
            
        dataframe.append(
            {
                'id' : paper['paper_id'],
                'author' : author['first'] + ' ' + author['last'],
                'country' : country,
                'institution' : institution
            }
        )
    
    return dataframe


#Nota: se la key c'è ma è vuota, metter Missing

authors = js.map(reformat).flatten().to_dataframe()
```

```{python}
authors.groupby('country').author.count().compute() #Solve the many Missing
```

```{python}
authors.groupby('institution').author.count().compute()
```

```{python}
#TO DO
# Gridsearch per i primi due punti => + Plot # Bea
# Plot (parole più frequenti, countries and institutions with more papers) # Marco
# Sistemare il grande numero di Missing, dovuto a diverse convenzioni # Marco
# nested try except # Marco
# Aggiungere commenti vari

#Terzo punto #Fra

```

## Embedding for the title of the papers
In NLP a common technique to perform analysis over a set of texts is to transform the text into a set of vectors, each one representing a word inside a document. At the end of the pre-processing, the document will be transformed into a list of vectors, or a matrix $n\times m$ where $n$ is the number of words in the document and $m$ is the size of the vector that represents each word. More information about word-embedding: https://towardsdatascience.com/introductionto-word-embedding-and-word2vec-652d0c2060fa

What you can do is to transform the **title** of each paper into its embedding version by using the pre-trained model available on the FastText page: https://fasttext.cc/docs/en/pretrainedvectors.html.
The pre-trained model that you have to download is the https://dl.fbaipublicfiles.com/fasttext/vectorswiki/wiki.en.vec

Basically the pre-trained model is more or less a huge dictionary in the following format `key: vector`. 

To load the model, follow the snippet of code which is slightly different from what you can find at this page: https://fasttext.cc/docs/en/english-vectors.html

```{python}
import io

def load_vectors(fname):
    fin = io.open(fname, ’r’, encoding=’utf-8’, newline=’\n’, errors=’ignore’)
    n, d = map(int, fin.readline().split())
    data = {}
    
    for line in fin:
        tokens = line.rstrip().split(’ ’)
        data[tokens[0]] = list(map(float, tokens[1:]))
        return data

model = load_vectors(’wiki.en.vec’)

#to get the embedding of word ’hello’:

model[’hello’]
```

Once you have downloaded the model, use the map approach to create a DataFrame or a Bag that is composed by:
- `paper-id`
- `title-embedding`

The title embedding can be a list of vectors or can be flattened to a large vector.

**Bonus point**
Use the previously generated vectors to compute the **cosine similarity** between each paper and to figure out a couple of papers with the highest cosine similairty score. This point is a bonus/optional point.

```{python}

```
