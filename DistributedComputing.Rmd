---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.4.1
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

## Word Counter

<!-- #region -->
First of all, you have to implement the following distributred algorithm to *count* the occurrences of all the words inside a list of documents. In NLP (Natural Language Processing) a document is a text; in this case, each paper is a document.

The algorithm is defined as follows:

- **Map phase**: For each document $D_i$, produce the set of intermediate pairs $(w, \mathrm{cp}_i(w))$, one for each word $w \in D_i$, where $\mathrm{cp}_i(w)$ is the number of occurrences of $w$ in $D_i$. E.g. $('\mathrm{hello}', 3)$.
- **Reduce phase**: For each word $w$, gather all the previous pairs $(w, \mathrm{cp}_i(w))$ and return the final pair $(w, \mathrm{c}(w))$ where $\mathrm{c}(w)$ is the number of occurrences of $w$ for all the documents. In other words:
$$ \mathrm{c}(w) = \sum_{k=1}^n \mathrm{cp}_k(w)$$


1. The algorithm has to be run on the **full-text** of the papers. To get the full text of a paper you have to transform the input data by concatenating the strings contained in the *body-text* fields of the JSON. To perform this transformation I strongly suggest you use the Bag data-structure of DASK. Anyway, if you prefer to implement the algorithm by using the DataFrame structure feel free to do it.

2. The algorithm has to be run several times by changing the *number of workers* and the *number of partitions*. For each run the **execution time** must be registered. Provide a comment on how the computation time over the cluster varies by changing the number of partitions/workers. You have to try with at least $6$ different partition numbers.

3. At the end of the algorithm, analyze the **top words** and see how they are related to viruses and research (for example create a barplot of the top words).

<!-- #endregion -->

```{python}
from dask.distributed import Client, LocalCluster
import dask.bag as db

import os
import json
import glob 

# Needed first time you use nltk
# import nltk as nlt
# nlt.download('stopwords')
import re
from nltk.corpus import stopwords

import matplotlib.pyplot as plt
import bokeh.palettes as palette
import seaborn as sn
import pandas as pd

from itertools import islice
import time 
import numpy as np

import warnings
warnings.filterwarnings('ignore')

import io
```

```{python}
client = Client()
client
```

```{python}
PATH = os.path.join('data', 'papers_in_json', '*.json')

js = db.read_text(PATH).map(json.loads)
```

```{python}
js = js.repartition(100)
```

```{python}
#string = ''.join([a['text'] for a in data['body_text']])

def joiner(paper):
    return ''.join([paragraph['text'] for paragraph in paper['body_text']])
```

```{python}
# Unire text in body-text
# Rimuovere punteggiatura etc.
# Contare parole (parola, conto)

filename = glob.glob(PATH)[0]

with open(filename) as f:
    data = json.loads(f.read())

string = ' '.join([a['text'] for a in data['body_text']]) 
```

```{python}
rm_word = ['also', 'may', 'et' , 'using', 'used', 'al', 'two', 'one', 'e', 'could', 'use']
stopw = stopwords.words('english') + rm_word

citations = re.compile('\[[\d]+\]')
numbers = re.compile('((^|\s)[\d]+)')
fixpoints = re.compile('([\.,:?;!](?=\w))')
urls = re.compile('(http(s)?:\/\/.)?(www\.)?[-a-zA-Z0-9@:%._\+~#=]{2,256}\.[a-z]{2,6}\b([-a-zA-Z0-9@:%_\+.~#?&//=]*)')
pattern = re.compile('[^a-zA-Z\d\s]')
spaces = re.compile('\s([\s]+)')
stop_words = re.compile(r'\b(' + r'|'.join(stopw) + r')\b\s*')


def sanitize_string(s):
    # Transform all capital letters in lower letters
    sanitized_string = s.lower()
    # Remove urls
    sanitized_string = urls.sub('', sanitized_string)
    # Remove citations of the type [x] where x is a number
    sanitized_string = citations.sub('', sanitized_string)
    # Insert spacing after puntuations 
    sanitized_string = fixpoints.sub(' ', sanitized_string)
    # Remove puntuation
    sanitized_string = pattern.sub('', sanitized_string)
    # Remove single numbers
    sanitized_string = numbers.sub('', sanitized_string)
    # Remove multiple spacing
    sanitized_string = spaces.sub('', sanitized_string)
    # Removing stopwords or words that aren't interesting to the analysis
    sanitized_string = stop_words.sub('', sanitized_string)
    
    return sanitized_string

s = sanitize_string(string)
```

```{python}
def count_words(s):
    wordlist = {}
    
    for word in s.split():
        if word in wordlist:
            wordlist[word] += 1
        else:
            wordlist[word] = 1
        
    return wordlist
```

```{python}
js = js.map(joiner)
js = js.map(sanitize_string).map(count_words)
```

```{python}
def merge_dictionaries(x, y):
    return {k: x.get(k, 0) + y.get(k, 0) for k in set(x) | set(y)}
    
reduction = js.fold(binop=merge_dictionaries, combine=merge_dictionaries)
```

```{python}
a = reduction.compute() #Fix error of plasmodium, should be 53 or 52
```

```{python}
words_count = {k: v for k, v in sorted(a.items(), key=lambda item: item[1], reverse=True)}
```

```{python}
# Present the first 10 words
idx = 10

best_words = dict( islice(words_count.items(), idx))

f, ax = plt.subplots(figsize=(12, 6))

ax.barh(list(best_words.keys())[::-1], list(best_words.values())[::-1], 
        color=palette.cividis(idx) )
ax.set_xlabel('Total')
ax.set_title('Most present words in the text')

plt.show()
```

```{python}
# GRIDSEARCH - TIME ANALYSIS

def compute_time(n_workers, n_partitions):
    
    cluster = LocalCluster(n_workers = n_workers)
    
    client = Client(cluster) 
    
    start = time.time() 
        
    PATH = os.path.join('data', 'papers_in_json', '*.json')

    js = db.read_text(PATH).map(json.loads)
    js = js.repartition(n_partitions)
    
    filename = glob.glob(PATH)[0]

    with open(filename) as f:
        data = json.loads(f.read())

    string = ' '.join([a['text'] for a in data['body_text']]) 
    
    s = sanitize_string(string)
    
    js = js.map(joiner)
    js = js.map(sanitize_string).map(count_words)
    
    reduction = js.fold(binop=merge_dictionaries, combine=merge_dictionaries)
    
    a = reduction.compute() #Fix error of plasmodium, should be 53 or 52

    words_count = {k: v for k, v in sorted(a.items(), key=lambda item: item[1], reverse=True)}

    end = time.time()
    
    client.close()
    cluster.close() 
    
    computed_time = end - start
    
    return computed_time
```

```{python}
workers = [1, 2, 3, 4]
partitions = [1, 2, 5, 10, 25, 100, 500]

times = []

for w in workers:
    for p in partitions:
        t = compute_time(n_workers = w, n_partitions = p*w)
        times.append(t)
        print("Workers=", w, "Partitions=", p*w, "Time=", t,"s")
```

```{python}
time_plot = np.array(times).reshape(len(workers), len(partitions))
```

```{python}
df_times = pd.DataFrame(time_plot)

plt.figure(figsize=(12,6))

ax = sn.heatmap(df_times, annot=True, annot_kws={"size": 14},fmt=".3f",
                xticklabels=partitions, yticklabels=[1,2,4],
                cbar_kws={'label':'Computation time [s]'},cmap='cividis_r' )
ax.invert_yaxis()
ax.set_xlabel("Partitions per workers", fontsize=14, fontweight='bold')
ax.set_ylabel("Worker", fontsize=14, fontweight='bold')

plt.show()
```

## Worst and Best represented Countries
1. In this part you have to take the documents and convert them in a usable *DataFrame* data structure in order to figure out the countries that are most and less active in the research. To do this you can use the country of the authors. Do the same for the universities (affiliations).

2. Even in this case, do multiple runs by changing the *number of partitions* and *workers* and then describe the behaviour of the timings.

```{python}
PATH = os.path.join('data', 'papers_in_json', '*.json')

js = db.read_text(PATH).map(json.loads)

#metadata => authors => affiliation => location => country

def reformat(paper):
    metadata = paper['metadata'] #dictionary
    
    dataframe = []
    
    for author in metadata['authors']:

        a = author['affiliation']
        
        try:
            country = a['location']['country']
            if  len(country)==0: # When country = {}
                country = 'Missing'
        except KeyError:
            try: # Try different position of the country
                country = a['location']['region']
            except KeyError:
                try:
                    country = a['location']['settlements']
                except KeyError:
                    country = 'Missing'
        
        try:
            institution = a['institution']
            if  len(institution)==0: # When institution = {}
                institution = 'Missing'
        except KeyError:
            institution = 'Missing'
            
        # We only store the informations that are interesting for our analysis
        dataframe.append(
            {
                'id' : paper['paper_id'],
                'author' : author['first'] + ' ' + author['last'],
                'country' : country,
                'institution' : institution
            }
        )
    
    return dataframe


authors = js.map(reformat).flatten().to_dataframe()
```

```{python}
# Group by country and then count. We select the author column aribitrarly, to have just
# a single-column serie
countries = authors.groupby('country').author.count().compute() 
sorted_countries = countries.sort_values(ascending=False) # Sorting in descending order
```

```{python}
f, ax = plt.subplots(figsize=(12, 6))
idx = 10

ax.barh(sorted_countries.index[idx:0:-1], sorted_countries.values[idx:0:-1],
        color=palette.cividis(idx) )
ax.set_xlabel('Total')
ax.set_title('Countries that published most papers')

plt.show()
```

```{python}
# GRIDSEARCH - TIME ANALYSIS

def compute_time_countries(n_workers, n_partitions):
    
    cluster = LocalCluster(n_workers = n_workers)
    
    client = Client(cluster) 
    
    start = time.time() 
        
    PATH = os.path.join('data', 'papers_in_json', '*.json')
    js = db.read_text(PATH).map(json.loads)
    js = js.repartition(n_partitions)
    
    authors = js.map(reformat).flatten().to_dataframe()
    
    countries = authors.groupby('country').author.count().compute() 
    sorted_countries = countries.sort_values(ascending=False) # Sorting in descending order
    
    end = time.time()
    
    client.close()
    cluster.close() 
    
    computed_time = end - start
    
    return computed_time
```

```{python}
workers = [1, 2, 3, 4]
partitions = [1, 2, 5, 10, 25, 100, 500]

times_c = []

for w in workers:
    for p in partitions:
        t = compute_time_countries(n_workers = w, n_partitions = p*w)
        times_c.append(t)
        print("Workers=", w, "Partitions=", p*w, "Time=", t,"s")
```

```{python}
# times_c.append(18)
time_plot_countries = np.array(times_c).reshape(len(workers), len(partitions))
```

```{python}
df_times_c = pd.DataFrame(time_plot_countries)

plt.figure(figsize=(12,6))

ax = sn.heatmap(df_times_c, annot=True, annot_kws={"size": 14},fmt=".3f",
                xticklabels=partitions, yticklabels=workers,
                cbar_kws={'label':'Computation time [s]'},cmap='cividis_r' )
ax.invert_yaxis()
ax.set_ylabel("Workers", fontsize=14, fontweight='bold')
ax.set_xlabel("Partitions per worker", fontsize=14, fontweight='bold')

plt.show()
```

```{python}
# Group by institution and then count. We select the author column aribitrarly, 
#to have just a single-column serie

universities = authors.groupby('institution').author.count().compute()
sorted_uni = universities.sort_values(ascending=False) # Sorting in descending order
```

```{python}
f, ax = plt.subplots(figsize=(12, 6))

idx = 10

ax.barh(sorted_uni.index[idx:0:-1], sorted_uni.values[idx:0:-1],
        color=palette.viridis(idx) )
ax.set_xlabel('Total')
ax.set_title('Universities that published most papers')

plt.show()
```

```{python}
# GRIDSEARCH - TIME ANALYSIS

def compute_time_uni(n_workers, n_partitions):
    
    cluster = LocalCluster(n_workers = n_workers)
    
    client = Client(cluster) 
    
    start = time.time() 
        
    PATH = os.path.join('data', 'papers_in_json', '*.json')
    js = db.read_text(PATH).map(json.loads)
    js = js.repartition(n_partitions)
    
    authors = js.map(reformat).flatten().to_dataframe()
    
    universities = authors.groupby('institution').author.count().compute()
    sorted_uni = universities.sort_values(ascending=False) # Sorting in descending order

    end = time.time()
    
    client.close()
    cluster.close() 
    
    computed_time = end - start
    
    return computed_time
```

```{python}
workers = [1, 2, 3, 4]
partitions = [1, 2, 5, 10, 25, 100, 500]

times_u = []

for w in workers:
    for p in partitions:
        t = compute_time_uni(n_workers = w, n_partitions = p*w)
        times_u.append(t)
        print("Workers=", w, "Partitions=", p*w, "Time=", t,"s")
```

```{python}
times_u.append(18)
time_plot_uni = np.array(times_u).reshape(len(workers), len(partitions))
```

```{python}
df_times_u = pd.DataFrame(time_plot_uni)

plt.figure(figsize=(12,6))

ax = sn.heatmap(df_times_u, annot=True, annot_kws={"size": 14},fmt=".3f",
                xticklabels=partitions, yticklabels=workers,
                cbar_kws={'label':'Computation time [s]'},cmap='cividis_r' )
ax.invert_yaxis()
ax.set_ylabel("Workers", fontsize=14, fontweight='bold')
ax.set_xlabel("Partitions per worker", fontsize=14, fontweight='bold')

plt.show()
```

```{python}
#TO DO
# Gridsearch per i primi due punti => + Plot # DONE
# Plot (parole più frequenti, countries and institutions with more papers) # DONE
# Sistemare il grande numero di Missing, dovuto a diverse convenzioni # 
# nested try except # DONE (Ancora un sacco di missing ma se i ricercatori son stupidi e 
# non scrivono da dove vengono se lo meritano)
# Aggiungere commenti vari

#Terzo punto #Fra

```

## Embedding for the title of the papers
In NLP a common technique to perform analysis over a set of texts is to transform the text into a set of vectors, each one representing a word inside a document. At the end of the pre-processing, the document will be transformed into a list of vectors, or a matrix $n\times m$ where $n$ is the number of words in the document and $m$ is the size of the vector that represents each word. More information about word-embedding: https://towardsdatascience.com/introductionto-word-embedding-and-word2vec-652d0c2060fa

What you can do is to transform the **title** of each paper into its embedding version by using the pre-trained model available on the FastText page: https://fasttext.cc/docs/en/pretrainedvectors.html.
The pre-trained model that you have to download is the https://dl.fbaipublicfiles.com/fasttext/vectorswiki/wiki.en.vec

Basically the pre-trained model is more or less a huge dictionary in the following format `key: vector`. 

To load the model, follow the snippet of code which is slightly different from what you can find at this page: https://fasttext.cc/docs/en/english-vectors.html

```{python}
def load_vectors(fname):
    fin = io.open(fname, ’r’, encoding=’utf-8’, newline=’\n’, errors=’ignore’)
    n, d = map(int, fin.readline().split())
    data = {}
    
    for line in fin:
        tokens = line.rstrip().split(’ ’)
        data[tokens[0]] = list(map(float, tokens[1:]))
        return data

model = load_vectors(’wiki.en.vec’)

#to get the embedding of word ’hello’:

model[’hello’]
```

Once you have downloaded the model, use the map approach to create a DataFrame or a Bag that is composed by:
- `paper-id`
- `title-embedding`

The title embedding can be a list of vectors or can be flattened to a large vector.

**Bonus point**
Use the previously generated vectors to compute the **cosine similarity** between each paper and to figure out a couple of papers with the highest cosine similairty score. This point is a bonus/optional point.

```{python}

```
