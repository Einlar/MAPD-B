---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.4.0
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# Theoretical analysis
We want to analyze the process of storing data in $N\in \mathbb{N}$ hard disks until one of them is full, i.e. contains $M\in \mathbb{N}$ data, choosing uniformly the disk where we store. We can store a quantity $h$ of data each time step, such that $q\cdot h=M$ with $q\in\mathbb{N}$. We can deal with this problem modeling it with a Markov Process. We will indicate our state as $\vec n=(n_1,\cdots, n_N)$ and with $\vec k = (0,\cdots,0,h,0,\cdots,0)$ the vector with $h$ in the $k$-th position and $0$ everywhere else.

We can write the transition rates as:
$$
\cases{W(\vec n|\vec{n'})=\frac{1}{N}\prod_i(1-\delta_{n_i',M}) & if $\vec{n'}=\vec n-\vec k$\\
W(\vec n|\vec{n'})=0 & if $\vec{n'}\neq\vec n-\vec k$
}
$$

We can so write the master equation associated to such a process:
$$
\frac{d}{dt}p(\vec n,t)=\frac{1}{N}\sum_k\prod_i\left[(1-\delta_{n_i-k,M})p(\vec n-\vec k, t)- (1-\delta_{n_i,M})p(\vec n, t) \right]
$$
Starting from this point we can find interesting quantities, such as:
\begin{align}
<n_j>&\stackrel{n_j<M}{=}\frac{h}{N}t\\
<|\vec n|^2>&\stackrel{n_j<M}{=}\frac{h^2}{6N}(N-12)t
\end{align}
From here we can see that, as long as $n_j\leq M$ $ \forall j$ both the quantity of data stored in a single disk and a measure of all the data stored in the system (the norm of the vector) scale linearly in time and both proportionally to the chunk size $h$.

But the really important result is obtaind by applying the Markov Inequality:
$$
p(n_j\geq M) \leq \frac{E(n_j)}{M}=\frac{h}{NM}t
$$
From here we can see that if $h<<M$ than $p(n_j\geq M)$ is really small, and so also the probability of stopping our process due to the filling of a single disk. Eventually one disk will be filled due to the proportionality with $t$, but in a much longer time that let us fill more evenly the disks.

A theoretical evaluation of $p(n_i|n_j\geq M)$ is beyond the scope of this introduction, but we will analyze it with numerical methods.


```{python}
import numpy as np
import matplotlib.pyplot as plt
```

```{python}
class CloudStorage():
    def __init__(self, N, M):
        """
        Params:
            N: int
                Number of disks available for the cloud storage
            M: int
                Memory available for each disk in GB
        """
        self.N = N
        self.disks = np.zeros(N)
        self.M = M
        self.OKtoGO = True        # Flag True if no disk is full
        self.files = 0            # Number of files in the storage
        
    def reset(self):
        self.__init__(self.N, self.M)
        
    def storage(self, h, warnings=False):
        """
        Params:
            h: float
                Chunk size of the data to be stored in GB
        Stores a file in the cloud storage if no one of the disk is full.
        """
        if any(self.disks >= self.M):
            if warnings:
                print('ERROR! One of the disk is full.' + 
                      ' The storage process is no longer possible.')
            self.OKtoGO = False
        elif self.OKtoGO:
            k = np.random.randint(0, self.N)
            if self.disks[k]+h > self.M:
                print('ERROR! Tried to store chunk size of '+ str(h) + ' GB in disk with only '
                      + str(self.M-self.disks[k]) + ' GB of space left.')
            else:
                self.disks[k] += h
                self.files += 1
                
    def plot(self):
        """
        Plots the distribution of the full space of the disks as barplot
        """
        fig, ax = plt.subplots( figsize=(12,6))
        ax.hlines(y=1, xmin=-1, xmax=10, linestyle='dashed', color='red')
        ax.bar(np.arange(self.N), self.disks/self.M)
        ax.set_ylabel('Relative space occupied')
        ax.set_xticks(np.arange(self.N))
        ax.set_xlabel('Disk number')
        plt.show()
        
    def result(self, prints=False):
        """
        Gives as an output the number of files stored and the average size of the empty
        space for each disk (excluded the full one).
        """
        perc = self.disks[self.disks != self.M]/self.M
        mean_perc = perc.mean()
        if prints:
            print('There are ' + str(self.files) + ' files allocated')
            print('The average allocation in the non-full disk is: '+ str(mean_perc*100) +
             ' %')
        return(self.files, mean_perc)
        
        
```

```{python}
GB10 = CloudStorage(10, 1e3)

while(GB10.OKtoGO):
    GB10.storage(10)
    
GB10.plot()
res = GB10.result(prints=True)
```

It does not make sense to evaluate the mean occupancy for each disk, since on average they will be the same, due to the randomness of the writing. However it can be interesting to have an average value of the number of files that can be stored in the disks and the average empty space per disk.

```{python}
def experiment(N, M, h, T):
    N_files = []
    Used_space = []
    CS = CloudStorage(N, M)
    
    for i in range(T):
        while(CS.OKtoGO):
            CS.storage(h)
        nf, us = CS.result()
        N_files.append( nf)
        Used_space.append( us)
        CS.reset()
    
    N_files = np.array(N_files)
    Used_space = np.array(Used_space)
    
    return(N_files.mean(), N_files.std(), Used_space.mean(), Used_space.std() )

```

```{python}
N_f_avg, N_f_std, U_s_avg, U_s_std = experiment(10, 1e3, 10, 100)
```

```{python}
print('The average number of files is: %i +- %i' %(N_f_avg, N_f_std))
print('The average relative used space is: %.2f +- %.2f'  %(U_s_avg, U_s_std))
```

We can now analyze the case with $1$ GB of file chunk

```{python}
N_f_avg, N_f_std, U_s_avg, U_s_std = experiment(10, 1e3, 1, 100)
print('The average number of files is: %i +- %i' %(N_f_avg, N_f_std))
print('The average relative used space is: %.2f +- %.2f'  %(U_s_avg, U_s_std))
```

We can now look at how the average used space evolves with the block size

```{python}
block_sizes = np.array([1, 2, 4, 5, 10, 20, 40, 100, 200, 500])
```

```{python}
U_S = []
U_S_std = []
for b in block_sizes:
    N_f_avg, N_f_std, U_s_avg, U_s_std = experiment(10, 1e3, b, 20)
    U_S.append(U_s_avg)
    U_S_std.append(U_s_std)
    
U_S_std = np.array(U_S_std)
U_S = np.array(U_S)
```

```{python}
fig, ax = plt.subplots(figsize=(12,6))
ax.plot(block_sizes/1e3, U_S, 'ro--', label='Data')
ax.fill_between(block_sizes/1e3, U_S+U_S_std, U_S-U_S_std, color='green', alpha=0.5,
                label='Standard devation')
ax.legend()
ax.set_xlabel('Relative block size')
ax.set_ylabel('Relative used space')
ax.set_title('Analysis of the average used space per disk wrt the block size')
plt.show()
```

Lastly we can analyze the distribution of the number of files accepted before filling a disk, and thus understand the time distribution of the filling.

```{python}
def time_distr(N, M, h, T):
    N_files = []
    CS = CloudStorage(N, M)
    
    for i in range(T):
        while(CS.OKtoGO):
            CS.storage(h)
        nf, us = CS.result()
        N_files.append( nf)
        CS.reset()
    
    N_files = np.array(N_files)
    
    return(N_files )

```

```{python}
distr10GB = time_distr(10, 1e3, 10, 1000)
distr1GB = time_distr(10, 1e3, 1, 1000)
```

```{python}
fig, ax = plt.subplots(figsize=(12,6))
ax.hist(distr10GB, alpha=0.8, color='blue', density=True, bins=20, label='h=10 GB')
ax.hist(distr1GB/10, alpha=0.8, color='green', density=True, bins=20, label='h=1 GB')
ax.set_xlabel('Number of files stored')
ax.set_ylabel('Density')
ax.set_title('Distribution of the number of files stored before '+
             'the end of the process with h=10GB')
ax.legend()
plt.show()
```

```{python}

```
